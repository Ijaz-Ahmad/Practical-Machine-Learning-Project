---
title: "Weight Lifting Exercises - Class Prediction"
author: "Ijaz Ahmad"
date: "16 July 2015"
output: html_document
---

## Executive Summary

- The human activity recognition research has traditionally focused on discriminating between different activities, i.e. to predict "which" activity was performed at a specific point in time. Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

- Our goal is to build a prediction model to identify the class of each observation based on the activity data recorded by researchers ("pml-training.csv" in our project). The prediction model built by us will be tested on each case of the data set "pml-testing.csv".

## Get/Load Data

- Download data from the specified URL and load into the working directory.

```{r Get.Load.Data, cache=TRUE, echo=FALSE}
## Download data

## download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
##               destfile = "~/Documents/Data/pml-training.csv")
## download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
##               destfile = "~/Documents/Data/pml-testing.csv")

## Load data

pml.training <- read.csv("~/Documents/Data/pml-training.csv")
pml.testing <- read.csv("~/Documents/Data/pml-testing.csv")
```
- Data set "pml-training" consists of `r dim(pml.training)[1]` observations and `r dim(pml.training)[2]` variables with only `r sum(complete.cases(pml.training))` complete cases.

## Exploratory Data Analysis

```{r Packages, echo=FALSE, message=FALSE}
## Load required packages

library(tidyr); library(caret); library(doMC)
registerDoMC(cores = 4)
```
- Load packages "caret", "tidyr" and "doMC".
- Register number of cores.
- Find near zero variance variables in "pml.training".
- Subset the training and testing data sets excluding the variables having near zero variance. This will help reducing execution time of algorithm and the noise in prediction process.
- Get rid of variables "X" and "user_name" being the row/observation identifier and name of the participant. Both the variables have no relationship with the prediction of outcome variable and will just add noise and bias in addition to higher execution time of the prediction model.
- Get rid of variables mostly having nil information ("NA"). The exclusion of such variables will improve our prediction accuracy a lot; because out of total number of `r dim(pml.training)[1]` observations we just have `r sum(complete.cases(pml.training))` complete cases. Also get rid of such variables from data set "pml.testing".

```{r Data.Analysis, cache=TRUE, echo=FALSE}
## Find Near Zoro Variance Variables

NZVs <- nearZeroVar(pml.training, saveMetrics = TRUE)

## subset the training and testing data set excluding the variables having near zero variance

pml.training <- pml.training[, which(NZVs$nzv == "FALSE")]
pml.testing <- pml.testing[, which(NZVs$nzv == "FALSE")]

## Get rid of variables "X" and "user_name" row/observaion identifier and name of the participant.

pml.training <- pml.training[, -c(1,2)]
pml.testing <- pml.testing[, -c(1,2)]

## Get rid of variables mostly having NAs

i = 1
NAsCount <- data.frame()
for(i in 1:dim(pml.training)[2]) {
        NAsCount <- rbind(NAsCount, length(which(is.na(pml.training[,i]))))
}
pml.training <- pml.training[, which(NAsCount == 0)]
pml.testing <- pml.testing[, which(NAsCount == 0)]
```

## Building our model

- We are now left with `r dim(pml.training)[1]` observations, `r dim(pml.training)[2]` variables with `r sum(complete.cases(pml.training))` complete cases to build our prediction model.
- Extract numeric values from all variables of both data sets except the outcome variable.
- Create training, testing and validation data sets by partitioning "pml.training".
- Build three models on data set training
        (i)   TREEBAG Model,
        (ii)  Random Forests Model "rf" and
        (iii) Boosting with trees Model "gbm".
- Combine the predictions of all three models along with the the true outcome on data set testing and fit a model on the combined data set.
```{r Partitioning, cache=TRUE, echo=FALSE}
## extract numeric values of all variables of data

i = 1
n <- dim(pml.training)[2] - 1
for(i in 1:n) {
        pml.training[,i] <- extract_numeric(pml.training[,i])
}

i = 1
n <- dim(pml.testing)[2] - 1
for(i in 1:n) {
        pml.testing[,i] <- extract_numeric(pml.testing[,i])
}

## Create training, testing and validation data sets

set.seed(1234)
inBuild <- createDataPartition(pml.training$classe, p = 0.7, list = FALSE)
BuildData <- pml.training[inBuild,]; validation <- pml.training[-inBuild,]
inTrain <- createDataPartition(BuildData$classe, p = 0.7, list = FALSE)
training <- BuildData[inTrain,]; testing <- BuildData[-inTrain,]
```

### TREEBAG Model
```{r Treebag, cache=TRUE, message=FALSE}
treebag.fit <- train(classe ~ ., data = training, method = "treebag", preProcess = c("center", "scale"))
```

### Random Forests Model "rf"
```{r RandomForest, cache=TRUE, message=FALSE}
rf.fit <- train(classe ~ ., data = training, method = "rf", preProcess = c("center", "scale"))
```

### Boosting with trees Model "gbm"
```{r BoostingWithTrees, cache=TRUE, message=FALSE}
gbm.fit <- train(classe ~ ., data = training, method = "gbm",
                 preProcess = c("center", "scale"), verbose = FALSE)
```

### Build combined model

- Combine predictions on test data as a data frame along with the true values of outcome.
- Build the combined model on the new data frame.

```{r CombinedData, cache=TRUE, echo=FALSE, message=FALSE}
## combine predictions into a data.frame

pred.treebag.test <- predict(treebag.fit, testing)
pred.rf.test <- predict(rf.fit, testing)
pred.gbm.test <- predict(gbm.fit, testing)
pred.comb.test <- data.frame(pred.treebag = pred.treebag.test,
                             pred.rf = pred.rf.test,
                             pred.gbm = pred.gbm.test, classe = testing$classe)
```

```{r CombinedModel, cache=TRUE, message=FALSE}
## Bulid combined model
comb.fit <- train(classe ~ ., data = pred.comb.test, method = "gbm")
```

#### Validate the model accuracy and root mean square error (RMSE) - test data

- The table showing the accuracy rate and RMSE for each model in respect of test data is:

```{r Table.Test.Results, cache=TRUE, echo=FALSE}
## predict outcome and calculate accuracy/RMSE in respect of each model
treebag.accuracy <- sum(pred.treebag.test == testing$classe) / nrow(testing)
treebag.RMSE <- sqrt((sum(as.numeric(pred.treebag.test) - as.numeric(testing$classe))^2) /
                             nrow(testing))

rf.accuracy <- sum(pred.rf.test == testing$classe) / nrow(testing)
rf.RMSE <- sqrt((sum(as.numeric(pred.rf.test) - as.numeric(testing$classe))^2) / nrow(testing))

gbm.accuracy <- sum(pred.gbm.test == testing$classe) / nrow(testing)
gbm.RMSE <- sqrt((sum(as.numeric(pred.gbm.test) - as.numeric(testing$classe))^2) / nrow(testing))

pred.test <- predict(comb.fit, pred.comb.test)
comb.accuracy <- sum(pred.test == pred.comb.test$classe) / nrow(pred.comb.test)
comb.RMSE <- sqrt((sum(as.numeric(pred.test) - as.numeric(pred.comb.test$classe))^2) /
                          nrow(pred.comb.test))

Acc.RMSE <- rbind(Accuracy = c(treebag = treebag.accuracy, rf = rf.accuracy,
                             gbm = gbm.accuracy, Combined = comb.accuracy),
              RMSE = c(treebag.RMSE, rf.RMSE, gbm.RMSE, comb.RMSE))
Acc.RMSE
```

#### Cross validate the model accuracy and root mean square error (RMSE) - validation data

- The table showing the accuracy rate and RMSE for each model in respect of validation data is:

```{r Table.Val.Results, echo=FALSE, message=FALSE}
## predict on validation data

pred.treebag.val <- predict(treebag.fit, validation)
pred.rf.val <- predict(rf.fit, validation)
pred.gbm.val <- predict(gbm.fit, validation)

## combine the predictions on validation data set

pred.comb.val <- data.frame(pred.treebag = pred.treebag.val,
                            pred.rf = pred.rf.val,
                            pred.gbm = pred.gbm.val)

val.pred <- predict(comb.fit, pred.comb.val)        ## predict on combined validation data set

## Calculate the accuracy and RMSE

val.treebag.accuracy <- sum(pred.treebag.val == validation$classe) / nrow(validation)
val.rf.accuracy <- sum(pred.rf.val == validation$classe) / nrow(validation)
val.gbm.accuracy <- sum(pred.gbm.val == validation$classe) / nrow(validation)
val.accuracy <- sum(val.pred == validation$classe) / nrow(validation)
val.treebag.RMSE <- sqrt((sum(as.numeric(pred.treebag.val) - as.numeric(validation$classe))^2) /
                          nrow(validation))
val.rf.RMSE <- sqrt((sum(as.numeric(pred.rf.val) - as.numeric(validation$classe))^2) /
                          nrow(validation))
val.gbm.RMSE <- sqrt((sum(as.numeric(pred.gbm.val) - as.numeric(validation$classe))^2) /
                          nrow(validation))
val.RMSE <- sqrt((sum(as.numeric(val.pred) - as.numeric(validation$classe))^2) /
                          nrow(validation))
## Tabulate the model accuracy and root mean square error (RMSE) on validation data

Acc.val.RMSE <- rbind(Accuracy = c(treebag = val.treebag.accuracy, rf = val.rf.accuracy,
                               gbm = val.gbm.accuracy, Combined = val.accuracy),
                  RMSE = c(val.treebag.RMSE, val.rf.RMSE, val.gbm.RMSE, val.RMSE))
Acc.val.RMSE
```

#### Reasons for model selection

##### 1.        TREEBAG

- TREEBAG is a system that allows to generate and transform objects of several types.
- It runs efficiently on large databases.
- It is very accurate.

##### 2.        Random Forest (rf)

- It is one of the most accurate learning algorithms available. For many data sets, it produces a highly accurate classifier.
- It runs efficiently on large databases.
- It can handle thousands of input variables without variable deletion.
- It generates an internal unbiased estimate of the generalization error as the forest building progresses.
- It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.
- It has methods for balancing error in class population unbalanced data sets.
- It offers an experimental method for detecting variable interactions.

##### 3.        Boosting with trees (gbm)

- Boosted regression trees incorporate important advantages of tree-based methods, handling different types of predictor variables and accommodating missing data.
- There is no need for prior data transformation or elimination of outliers.
- Can fit complex nonlinear relationships.
- Automatically handles interaction effects between predictors.

##### 4.        Combining prediction models

- I have selected again the method of random forest (rf) in train function to fit the combined model for further prediction of outcome variable (classe) for the combined data frame of all three prediction models along with the true values of variable "classe" for data set "testing".
- It is one of the most accurate learning algorithms available and runs efficiently on large databases.

#### The table of accuracy levels and plots for all four models still support the choice Random Forest Model.

```{r AccuracyComparison, echo=FALSE}
Acc.test.val <- rbind(Test = c(treebag = treebag.accuracy, rf = rf.accuracy,
                             gbm = gbm.accuracy, Combined = comb.accuracy),
                      Validation = c(val.treebag.accuracy, val.rf.accuracy,
                               val.gbm.accuracy, val.accuracy))
Acc.test.val
```


#### Plot Model Predictions

- The miss classified portion of data in respect of each model is almost invisible because of negligible in-sample and out-of-sample error rate.

```{r test.plot, echo=FALSE, fig.height=7, fig.width=9, fig.align='center', message=FALSE}
library(gridExtra)
p1 <- qplot(pred.treebag.test, data = testing, fill = classe, geom = "bar",
            xlab = "treebag Predictions")
p2<- qplot(pred.rf.test, data = testing, fill = classe, geom = "bar", xlab = "rf Predictions")
p3 <- qplot(pred.gbm.test, data = testing, fill = classe, geom = "bar", xlab = "gbm Predictions")
p4 <- qplot(pred.test, data = pred.comb.test, fill = classe, geom = "bar",xlab = "Combined Predictions")
grid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2,
             main = "Fig 1: Classification / Misclassification - Test Data")
```


```{r val.plot, echo=FALSE, fig.height=7, fig.width=9, fig.align='center', message=FALSE}
p5 <- qplot(pred.treebag.val, data = validation, fill = classe, geom = "bar",
            xlab = "treebag Predictions")
p6 <- qplot(pred.rf.val, data = validation, fill = classe, geom = "bar", xlab = "rf Predictions")
p7 <- qplot(pred.gbm.val, data = validation, fill = classe, geom = "bar", xlab = "gbm Predictions")
p8 <- qplot(val.pred, data = validation, fill = classe, geom = "bar",xlab = "Combined Predictions")
grid.arrange(p5, p6, p7, p8, nrow = 2, ncol = 2,
             main = "Fig 2: Classification / Misclassification - Validation Data")
```


#### In-sample and out-of-sample error rate

```{r out-of-sample-error, echo=FALSE}
Error_Rate <- rbind(in_sample = c(treebag = paste0(round((1-treebag.accuracy) * 100, 2), "%"),
                                      rf = paste0(round((1-rf.accuracy) * 100, 2), "%"),
                                      gbm = paste0(round((1-gbm.accuracy) * 100, 2), "%"),
                                      Combined = paste0(round((1-comb.accuracy) * 100, 2), "%")),
                    out_of_sample = c(paste0(round((1-val.treebag.accuracy) * 100, 2), "%"),
                                      paste0(round((1-val.rf.accuracy) * 100, 2), "%"),
                                      paste0(round((1-val.gbm.accuracy) * 100, 2), "%"),
                                      paste0(round((1-val.accuracy) * 100, 2), "%")))
```

- In my opinion in-sample error rate is the error rate in respect of incorrect predictions made by the model on test data set; while out-of-sample error rate is eventually the error rate in respect of incorrect predictions made by the model on validation data set. In other words, in-sample error rate is the result of model's validation and out-of-sample error rate is the result of model's cross-validation.
- **Random Forest Model** has produced least out-of-sample error rate and is, therefore, our final model.

```{r ErrorRates, echo=FALSE}
print(Error_Rate, quote = FALSE)
```


#### Write the case predictions

- Predict the outcome on 20 different cases of data set "pml.testing".
- Create one file for each case submission.
```{r Prediction.Write-up, echo=FALSE}

pred.rf.cases <- predict(rf.fit, pml.testing)
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

## Write one file for each case

pml_write_files(pred.rf.cases)
```

